# Home_Sales

This code shows a simple Spark setup, including reading a csv file into a Spark dataframe, querying the dataframe to output certain desired results, caching the temporary table to increase query speeds and partitioning the data in parquet format to read back into a dataframe and query that way.

- Much of the code was inspired by the previous activities done in the Big Data module. 